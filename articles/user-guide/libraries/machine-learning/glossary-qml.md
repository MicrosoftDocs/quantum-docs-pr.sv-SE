---
title: Bibliotek för kvantmaskininlärning
author: alexeib2
ms.author: alexei.bocharov@microsoft.com
ms.date: 2/27/2020
ms.topic: article
uid: microsoft.quantum.libraries.machine-learning.training
no-loc:
- Q#
- $$v
ms.openlocfilehash: 52c3f69fb99384270a27e57c4f32212d18bee1a4
ms.sourcegitcommit: 6bf99d93590d6aa80490e88f2fd74dbbee8e0371
ms.translationtype: MT
ms.contentlocale: sv-SE
ms.lasthandoff: 08/06/2020
ms.locfileid: "87868907"
---
# <a name="quantum-machine-learning-glossary"></a><span data-ttu-id="1a021-102">Quantum Machine Learning-ordlista</span><span class="sxs-lookup"><span data-stu-id="1a021-102">Quantum Machine Learning glossary</span></span>

<span data-ttu-id="1a021-103">Utbildning av en krets-centrerad Quantum-klassificerare är en process med många rörliga delar som kräver samma (eller något större) mängd kalibrering per utvärderings version och fel som utbildning av traditionella klassificerare.</span><span class="sxs-lookup"><span data-stu-id="1a021-103">Training of a circuit-centric quantum classifier is a process with many moving parts that require the same (or slightly larger) amount of calibration by trial and error as training of traditional classifiers.</span></span> <span data-ttu-id="1a021-104">Här definierar vi huvud begreppen och ingredienserna i den här utbildnings processen.</span><span class="sxs-lookup"><span data-stu-id="1a021-104">Here we define the main concepts and ingredients of this training process.</span></span>

## <a name="trainingtesting-schedules"></a><span data-ttu-id="1a021-105">Utbildnings-/testnings scheman</span><span class="sxs-lookup"><span data-stu-id="1a021-105">Training/testing schedules</span></span>

<span data-ttu-id="1a021-106">I samband med klassificerings träning beskriver ett *schema* en delmängd av data exempel i en övergripande utbildning eller testnings uppsättning.</span><span class="sxs-lookup"><span data-stu-id="1a021-106">In the context of classifier training a *schedule* describes a subset of data samples in an overall training or testing set.</span></span> <span data-ttu-id="1a021-107">Ett schema definieras vanligt vis som en samling av exempel index.</span><span class="sxs-lookup"><span data-stu-id="1a021-107">A schedule is usually defined as a collection of sample indices.</span></span>

## <a name="parameterbias-scores"></a><span data-ttu-id="1a021-108">Parameter/bias-Poäng</span><span class="sxs-lookup"><span data-stu-id="1a021-108">Parameter/bias scores</span></span>

<span data-ttu-id="1a021-109">Med hänsyn till en kandidat parameter vektor och en klassificerings kompensation mäts deras *validerings Poäng* i förhållande till det valda verifierings schemat och uttrycks av ett antal felklassificeringar som räknas över alla exempel i schemat.</span><span class="sxs-lookup"><span data-stu-id="1a021-109">Given a candidate parameter vector and a classifier bias, their *validation score* is measured relative to a chosen validation schedule S and is expressed by a number of misclassifications counted over all the samples in the schedule S.</span></span>

## <a name="hyperparameters"></a><span data-ttu-id="1a021-110">Hyperparametrar</span><span class="sxs-lookup"><span data-stu-id="1a021-110">Hyperparameters</span></span>

<span data-ttu-id="1a021-111">Modell inlärnings processen styrs av vissa fördefinierade värden som kallas för mina *parametrar*:</span><span class="sxs-lookup"><span data-stu-id="1a021-111">The model training process is governed by certain pre-set values called *hyperparameters*:</span></span>

### <a name="learning-rate"></a><span data-ttu-id="1a021-112">Inlärningstakt</span><span class="sxs-lookup"><span data-stu-id="1a021-112">Learning rate</span></span>

<span data-ttu-id="1a021-113">Det är en av nyckelns egenskaper.</span><span class="sxs-lookup"><span data-stu-id="1a021-113">It is one of the key hyperparameters.</span></span> <span data-ttu-id="1a021-114">Den definierar hur mycket aktuell Stochastic gradient-uppskattning påverkar parametern Update.</span><span class="sxs-lookup"><span data-stu-id="1a021-114">It defines how much current stochastic gradient estimate impacts the parameter update.</span></span> <span data-ttu-id="1a021-115">Storleken på parameter uppdaterings förändring är proportionell mot inlärnings takten.</span><span class="sxs-lookup"><span data-stu-id="1a021-115">The size of parameter update delta is proportional to the learning rate.</span></span> <span data-ttu-id="1a021-116">Lägre inlärnings frekvens värden leder till långsammare parameter utveckling och långsammare konvergens, men alltför stora värden för LR kan bryta konvergensen helt eftersom övertonings brantaste aldrig allokeras till ett visst lokalt minimum.</span><span class="sxs-lookup"><span data-stu-id="1a021-116">Smaller learning rate values lead to slower parameter evolution and slower convergence, but excessively large values of LR may break the convergence altogether as the gradient descent never commits to a particular local minimum.</span></span> <span data-ttu-id="1a021-117">Medan inlärnings takten justeras anpassningsbart av utbildningen i viss utsträckning, är det viktigt att välja ett bra initialt värde för det.</span><span class="sxs-lookup"><span data-stu-id="1a021-117">While learning rate is adaptively adjusted by the training algorithm to some extent, selecting a good initial value for it is important.</span></span> <span data-ttu-id="1a021-118">Ett vanligt standard start värde för inlärnings pris är 0,1.</span><span class="sxs-lookup"><span data-stu-id="1a021-118">A usual default initial value for learning rate is 0.1.</span></span> <span data-ttu-id="1a021-119">Att välja det bästa värdet för inlärnings takt är en bra bild (se till exempel Section 4,3 av Goodfellow et al., "djup inlärning", MIT Press, 2017).</span><span class="sxs-lookup"><span data-stu-id="1a021-119">Selecting the best value of learning rate is a fine art (see, for example, section 4.3 of Goodfellow et al.,"Deep learning", MIT Press, 2017).</span></span>

### <a name="minibatch-size"></a><span data-ttu-id="1a021-120">Minibatch-storlek</span><span class="sxs-lookup"><span data-stu-id="1a021-120">Minibatch size</span></span>

<span data-ttu-id="1a021-121">Definierar hur många data exempel som används för en enskild uppskattning av Stochastic-toning.</span><span class="sxs-lookup"><span data-stu-id="1a021-121">Defines how many data samples is used for a single estimation of stochastic gradient.</span></span> <span data-ttu-id="1a021-122">Större värden av minibatch-storlek leder vanligt vis till mer robust och mer enkel och mer högfärgad konvergens, men kan eventuellt sakta ned inlärnings processen, eftersom kostnaden för en övertoning är proportionell mot minimatch-storleken.</span><span class="sxs-lookup"><span data-stu-id="1a021-122">Larger values of minibatch size generally lead to more robust and more monotonic convergence but can potentially slow down the training process, as the cost of any one gradient estimation is proportional to the minimatch size.</span></span> <span data-ttu-id="1a021-123">Ett vanligt standardvärde för minibatch-storleken är 10.</span><span class="sxs-lookup"><span data-stu-id="1a021-123">A usual default value for the minibatch size is 10.</span></span>

### <a name="training-epochs-tolerance-gridlocks"></a><span data-ttu-id="1a021-124">Utbildnings epoker, tolerans, gridlocks</span><span class="sxs-lookup"><span data-stu-id="1a021-124">Training epochs, tolerance, gridlocks</span></span>

<span data-ttu-id="1a021-125">"Epok" syftar på ett komplett steg genom de schemalagda tränings data.</span><span class="sxs-lookup"><span data-stu-id="1a021-125">"Epoch" means one complete pass through the scheduled training data.</span></span>
<span data-ttu-id="1a021-126">Det maximala antalet epoker per utbildnings tråd (se nedan) bör vara ett tak.</span><span class="sxs-lookup"><span data-stu-id="1a021-126">The maximum number of epochs per a training thread (see below) should be capped.</span></span> <span data-ttu-id="1a021-127">Inlärnings tråden definieras för att avslutas (med de bästa kända kandidat parametrarna) när det maximala antalet epoker har körts.</span><span class="sxs-lookup"><span data-stu-id="1a021-127">The training thread is defined to terminate (with the best known candidate parameters) when the maximum number of epochs has been executed.</span></span> <span data-ttu-id="1a021-128">En sådan utbildning skulle dock upphöra tidigare när klassificerings schemats klassificerings schema sjunker under en vald tolerans.</span><span class="sxs-lookup"><span data-stu-id="1a021-128">However such training would terminate earlier when misclassification rate on validation schedule falls below a chosen tolerance.</span></span> <span data-ttu-id="1a021-129">Anta till exempel att toleransen för avvikelser är 0,01 (1%). om verifierings uppsättningen av 2000-prov visas färre än 20 felklassificeringar har tolerans nivån uppnåtts.</span><span class="sxs-lookup"><span data-stu-id="1a021-129">Suppose, for example, that misclassification tolerance is 0.01 (1%); if on validation set of 2000 samples we are seeing fewer than 20 misclassifications, then the tolerance level has been achieved.</span></span> <span data-ttu-id="1a021-130">En utbildnings tråd avslutas också för tidigt om validerings poängen för kandidat modellen inte har visat någon förbättring över flera på varandra följande epoker (en gridlock).</span><span class="sxs-lookup"><span data-stu-id="1a021-130">A training thread also terminates prematurely if the validation score of the candidate model has not shown any improvement over several consecutive epochs (a gridlock).</span></span> <span data-ttu-id="1a021-131">Logiken för gridlock-avslutningen är för närvarande hårdkodad.</span><span class="sxs-lookup"><span data-stu-id="1a021-131">The logic for the gridlock termination is currently hardcoded.</span></span>

### <a name="measurements-count"></a><span data-ttu-id="1a021-132">Antal mätningar</span><span class="sxs-lookup"><span data-stu-id="1a021-132">Measurements count</span></span>

<span data-ttu-id="1a021-133">Uppskatta inlärnings-och validerings poängen och komponenterna i Stochastic-övertoningen på en Quantum-enhets mängder för att uppskatta Quantum State-överlappningar som kräver flera mätningar av lämplig observables.</span><span class="sxs-lookup"><span data-stu-id="1a021-133">Estimating the training/validation scores and the components of the stochastic gradient on a quantum device amounts to estimating quantum state overlaps that requires multiple measurements of the appropriate observables.</span></span> <span data-ttu-id="1a021-134">Antalet mätningar ska skalas som $O (1/\ Epsilon ^ 2) $ där $ \epsilon $ är det önskade uppskattnings felet.</span><span class="sxs-lookup"><span data-stu-id="1a021-134">The number of measurements should scale as $O(1/\epsilon^2)$ where $\epsilon$ is the desired estimation error.</span></span>
<span data-ttu-id="1a021-135">Som en tumregel kan det första antalet mätningar vara cirka $1/\ mbox {tolerans} ^ 2 $ (se definitionen av toleransen i föregående stycke).</span><span class="sxs-lookup"><span data-stu-id="1a021-135">As a rule of thumb, the initial measurements count could be approximately $1/\mbox{tolerance}^2$ (see definition of tolerance in the previous paragraph).</span></span> <span data-ttu-id="1a021-136">Du skulle behöva ändra mätnings antalet uppåt om tonings brantaste verkar vara alltför oförutsägbart och konvergensen för svår att uppnå.</span><span class="sxs-lookup"><span data-stu-id="1a021-136">One would need to revise the measurement count upward if the gradient descent appears to be too erratic and convergence too hard to achieve.</span></span>

### <a name="training-threads"></a><span data-ttu-id="1a021-137">Utbildnings trådar</span><span class="sxs-lookup"><span data-stu-id="1a021-137">Training threads</span></span>

<span data-ttu-id="1a021-138">Sannolikhets funktionen för klassificeraren är mycket sällan konvex, vilket innebär att det vanligt vis har en mängd lokala OPTIMA i parameter utrymmet som kan variera avsevärt beroende på kvalitet.</span><span class="sxs-lookup"><span data-stu-id="1a021-138">The likelihood function which is the training utility for the classifier is very seldom convex, meaning that it usually has a multitude of local optima in the parameter space that may differ significantly by quality.</span></span> <span data-ttu-id="1a021-139">Eftersom SGD-processen kan konvergera enbart till en särskilt optimal, är det viktigt att utforska flera inledande parameter vektorer.</span><span class="sxs-lookup"><span data-stu-id="1a021-139">Since the SGD process can converge to only one specific optimum, it is important to explore multiple starting parameter vectors.</span></span> <span data-ttu-id="1a021-140">Vanliga metoder i Machine Learning är att initiera sådana start vektorer slumpmässigt.</span><span class="sxs-lookup"><span data-stu-id="1a021-140">Common practice in machine learning is to initialize such starting vectors randomly.</span></span> <span data-ttu-id="1a021-141">Q#Utbildnings-API: et accepterar en godtycklig matris av sådana start vektorer, men den underliggande koden utforskar dem i turordning.</span><span class="sxs-lookup"><span data-stu-id="1a021-141">The Q# training API accepts an arbitrary array of such starting vectors but the underlying code explores them sequentially.</span></span> <span data-ttu-id="1a021-142">På en dator med flera kärnor eller i själva verket för en parallell data behandling är det lämpligt att utföra flera anrop till Q# Training API parallellt med olika parameter initieringar i anropen.</span><span class="sxs-lookup"><span data-stu-id="1a021-142">On a multicore computer or in fact on any parallel computing architecture it is advisable to perform several calls to Q# training API in parallel with different parameter initializations across the calls.</span></span>

#### <a name="how-to-modify-the-hyperparameters"></a><span data-ttu-id="1a021-143">Ändra de båda parametrarna</span><span class="sxs-lookup"><span data-stu-id="1a021-143">How to modify the hyperparameters</span></span>

<span data-ttu-id="1a021-144">I QML-biblioteket är det bästa sättet att ändra de båda parametrarna genom att åsidosätta standardvärdena för UDT [`TrainingOptions`](xref:microsoft.quantum.machinelearning.trainingoptions) .</span><span class="sxs-lookup"><span data-stu-id="1a021-144">In the QML library, the best way to modify the hyperparameters is by overriding the default values of the UDT [`TrainingOptions`](xref:microsoft.quantum.machinelearning.trainingoptions).</span></span> <span data-ttu-id="1a021-145">För att göra detta anropar vi den med funktionen [`DefaultTrainingOptions`](xref:microsoft.quantum.machinelearning.defaulttrainingoptions) och använder operatorn `w/` för att åsidosätta standardvärdena.</span><span class="sxs-lookup"><span data-stu-id="1a021-145">To do this we call it with the function [`DefaultTrainingOptions`](xref:microsoft.quantum.machinelearning.defaulttrainingoptions) and apply the operator `w/` to override the default values.</span></span> <span data-ttu-id="1a021-146">Om du till exempel vill använda 100 000-mått och en inlärnings grad på 0,01:</span><span class="sxs-lookup"><span data-stu-id="1a021-146">For example, to use 100,000 measurements and a learning rate of 0.01:</span></span>
 ```qsharp
let options = DefaultTrainingOptions()
w/ LearningRate <- 0.01
w/ NMeasurements <- 100000;
 ```
