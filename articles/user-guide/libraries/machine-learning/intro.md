---
title: Bibliotek för kvantmaskininlärning
description: Lär dig hur Machine Learning används på Quantum Systems
author: alexeib2
ms.author: alexeib
ms.date: 11/22/2019
ms.topic: article
uid: microsoft.quantum.libraries.machine-learning.intro
no-loc:
- Q#
- $$v
ms.openlocfilehash: 9f7f892fb2b76432942c86163497c22f0c73d51f
ms.sourcegitcommit: 9b0d1ffc8752334bd6145457a826505cc31fa27a
ms.translationtype: MT
ms.contentlocale: sv-SE
ms.lasthandoff: 09/21/2020
ms.locfileid: "90833803"
---
# <a name="introduction-to-quantum-machine-learning"></a><span data-ttu-id="8124f-103">Introduktion till Quantum Machine Learning</span><span class="sxs-lookup"><span data-stu-id="8124f-103">Introduction to Quantum Machine Learning</span></span>

## <a name="framework-and-goals"></a><span data-ttu-id="8124f-104">Ramverk och mål</span><span class="sxs-lookup"><span data-stu-id="8124f-104">Framework and goals</span></span>

<span data-ttu-id="8124f-105">Quantum encoding och bearbetning av information är ett kraftfullt alternativ till klassisk Machine Learning Quantum-klassificerare.</span><span class="sxs-lookup"><span data-stu-id="8124f-105">Quantum encoding and processing of information is a powerful alternative to classical machine learning Quantum classifiers.</span></span> <span data-ttu-id="8124f-106">I synnerhet gör det möjligt för oss att koda data i Quantum-register som är koncisa i förhållande till antalet funktioner, som systematiskt använder Quantum entanglement som beräknings resurs och använder Quantum-mätning för klass härledning.</span><span class="sxs-lookup"><span data-stu-id="8124f-106">In particular, it allows us to encode data in quantum registers that are concise relative to the number of features, systematically employing quantum entanglement as computational resource and employing quantum measurement for class inference.</span></span>
<span data-ttu-id="8124f-107">Krets centrerad Quantum-klassificerare är en relativt enkel Quantum-lösning som kombinerar data kodning med en snabbt Entangling/disentangling Quantum-krets följt av mått för att härleda klass etiketter för data exempel.</span><span class="sxs-lookup"><span data-stu-id="8124f-107">Circuit centric quantum classifier is a relatively simple quantum solution that combines data encoding with a rapidly entangling/disentangling quantum circuit followed by measurement to infer class labels of data samples.</span></span>
<span data-ttu-id="8124f-108">Målet är att säkerställa klassisk karakterisering och lagring av ämnes kretsar, samt hybrid-och klassisk utbildning för krets parametrarna, även för mycket stora funktions utrymmen.</span><span class="sxs-lookup"><span data-stu-id="8124f-108">The goal is to ensure classical characterization and storage of subject circuits, as well as hybrid quantum/classical training of the circuit parameters even for extremely large feature spaces.</span></span>

## <a name="classifier-architecture"></a><span data-ttu-id="8124f-109">Klassificerings arkitektur</span><span class="sxs-lookup"><span data-stu-id="8124f-109">Classifier architecture</span></span>

<span data-ttu-id="8124f-110">Klassificering är en övervakad maskin inlärnings uppgift där målet är att härleda klass etiketter $ \{ y_1, y_2, \ldots, y_d \} $ av vissa data exempel.</span><span class="sxs-lookup"><span data-stu-id="8124f-110">Classification is a supervised machine learning task, where the goal is to infer class labels $\{y_1,y_2,\ldots,y_d\}$ of certain data samples.</span></span> <span data-ttu-id="8124f-111">"Tränings data uppsättning" är en samling exempel $ \mathcal{D} = \{ (x, y)} $ med kända fördefinierade etiketter.</span><span class="sxs-lookup"><span data-stu-id="8124f-111">The "training data set" is a collection of samples $\mathcal{D}=\{(x,y)}$ with known pre-assigned labels.</span></span> <span data-ttu-id="8124f-112">Här $x $ är ett data exempel och $y $ är dess kända etikett som kallas "övnings etikett".</span><span class="sxs-lookup"><span data-stu-id="8124f-112">Here $x$ is a data sample and $y$ is its known label called "training label".</span></span>
<span data-ttu-id="8124f-113">I likhet med traditionella metoder består Quantum klassificering av tre steg:</span><span class="sxs-lookup"><span data-stu-id="8124f-113">Somewhat similar to traditional methods, quantum classification consists of three steps:</span></span>
- <span data-ttu-id="8124f-114">data kodning</span><span class="sxs-lookup"><span data-stu-id="8124f-114">data encoding</span></span>
- <span data-ttu-id="8124f-115">förberedelse av ett klassificerings tillstånd</span><span class="sxs-lookup"><span data-stu-id="8124f-115">preparation of a classifier state</span></span>
- <span data-ttu-id="8124f-116">mått på grund av måttets Probabilistic karaktär måste dessa tre steg upprepas flera gånger.</span><span class="sxs-lookup"><span data-stu-id="8124f-116">measurement Due to the probabilistic nature of the measurement, these three steps must be repeated multiple times.</span></span> <span data-ttu-id="8124f-117">Både kodningen och bearbetningen av klassificerings statusen görs med hjälp av *Quantum-kretsar*.</span><span class="sxs-lookup"><span data-stu-id="8124f-117">Both the encoding and the computing of the classifier state are done by means of *quantum circuits*.</span></span> <span data-ttu-id="8124f-118">Även om kodnings kretsen vanligt vis är data driven och parameter fri, innehåller klassificerings kretsen en tillräckligt stor uppsättning parametrar.</span><span class="sxs-lookup"><span data-stu-id="8124f-118">While the encoding circuit is usually data-driven and parameter-free, the classifier circuit contains a sufficient set of learnable parameters.</span></span> 

<span data-ttu-id="8124f-119">I den föreslagna lösningen består klassificerings kretsen av en qubit rotation och två qubit kontrollerade rotationer.</span><span class="sxs-lookup"><span data-stu-id="8124f-119">In the proposed solution the classifier circuit is composed of single-qubit rotations and two-qubit controlled rotations.</span></span> <span data-ttu-id="8124f-120">De lär sig parametrarna här är rotations vinklarna.</span><span class="sxs-lookup"><span data-stu-id="8124f-120">The learnable parameters here are the rotation angles.</span></span> <span data-ttu-id="8124f-121">Rotations-och kontrollerade rotations grindarna har visat sig vara *universella* för Quantum-beräkning, vilket innebär att en enhetlig vikt mat ris kan delas upp i en tillräckligt lång krets som består av sådana grindar.</span><span class="sxs-lookup"><span data-stu-id="8124f-121">The rotation and controlled rotation gates are known to be *universal* for quantum computation, which means that any unitary weight matrix can be decomposed into a long enough circuit consisting of such gates.</span></span>

<span data-ttu-id="8124f-122">I den föreslagna versionen stöds endast en krets följt av en beräkning med en frekvens.</span><span class="sxs-lookup"><span data-stu-id="8124f-122">In the proposed version, only one circuit followed by a single frequency estimation is supported.</span></span>
<span data-ttu-id="8124f-123">Det innebär att lösningen är en Quantum analog av en support vektor dator med en mindre grad polynom kernel.</span><span class="sxs-lookup"><span data-stu-id="8124f-123">Thus, the solution is a quantum analog of a support vector machine with a low-degree polynomial kernel.</span></span>

![Multilayer Perceptron vs. krets-inriktad klassificerare](~/media/DLvsQCC.png)

<span data-ttu-id="8124f-125">En enkel Quantum klassificerare-design kan jämföras med en vanlig SVM-lösning (support Vector Machine).</span><span class="sxs-lookup"><span data-stu-id="8124f-125">A simple quantum classifier design can be compared to a traditional support vector machine (SVM) solution.</span></span> <span data-ttu-id="8124f-126">Den här härledningen för ett data exempel $x $ i händelse av SVM görs med ett optimalt kernel-formulär $ \sum \ alpha_j k (x_j, x) $ där $k $ är en viss kernel-funktion.</span><span class="sxs-lookup"><span data-stu-id="8124f-126">The inference for a data sample $x$ in case of SVM is done using an optimal kernel form $\sum \alpha_j  k(x_j,x)$ where $k$ is a certain kernel function.</span></span>

<span data-ttu-id="8124f-127">Däremot använder en Quantum-klassificeraren $p (y │ x, U (\theta)) = 〈 U (\theta) x | M | U (\theta) x 〉 $, som liknar sprit men tekniskt ganska annorlunda.</span><span class="sxs-lookup"><span data-stu-id="8124f-127">By contrast, a quantum classifier uses the predictor $p(y│x,U(\theta))=〈U(\theta)x|M|U(\theta)x〉$, which is similar in spirit but technically quite different.</span></span> <span data-ttu-id="8124f-128">Det innebär att när en enkel amplitud-kodning används, $p (y │ x, U (\theta)) $ är en kvadratisk form i amplituderna för $x $, men koefficienterna för det här formuläret har inte längre lärts oberoende. de sammanställs i stället från mat ris elementen i krets $U (\theta) $, som vanligt vis har betydligt mindre lärt parametrar $ \theta $ än dimensionerna i Vector $x $.</span><span class="sxs-lookup"><span data-stu-id="8124f-128">Thus, when a straightforward amplitude encoding is used,  $p(y│x,U(\theta))$ is a quadratic form in the amplitudes of $x$, but the coefficients of this form are no longer learned independently; they are instead aggregated from the matrix elements of the circuit $U(\theta)$, which typically has significantly fewer learnable parameters $\theta$ than the dimension of the vector $x$.</span></span> <span data-ttu-id="8124f-129">Polynom graden av $p (y │ x, U (\theta)) $ i de ursprungliga funktionerna kan höjas till $2 ^ l $ genom att använda en Quantum produkt kodning på $l $ kopior av $x $.</span><span class="sxs-lookup"><span data-stu-id="8124f-129">The polynomial degree of $p(y│x,U(\theta))$ in the original features can be increased to $2^l$ by using a quantum product encoding on $l$ copies of $x$.</span></span>

<span data-ttu-id="8124f-130">Vår arkitektur utforskar relativt lite-kretsar som därför måste vara *snabbt Entangling* för att kunna samla in alla korrelationer mellan data funktionerna i alla intervall.</span><span class="sxs-lookup"><span data-stu-id="8124f-130">Our architecture explores relatively shallow circuits, which therefore must be *rapidly entangling* in order to capture all the correlations between the data features at all ranges.</span></span> <span data-ttu-id="8124f-131">Ett exempel på den mest användbara Entangling krets-komponenten visas på bilden nedan.</span><span class="sxs-lookup"><span data-stu-id="8124f-131">An example of the most useful rapidly entangling circuit component is shown on figure below.</span></span> <span data-ttu-id="8124f-132">Även om en krets med den här geometrin bara består av $3 n + 1 $ grindar, säkerställer den enhetliga vikt mat ris som den beräknar betydande kors samtal mellan $2 ^ n $-funktioner.</span><span class="sxs-lookup"><span data-stu-id="8124f-132">Even though a circuit with this geometry consists of only $3 n+1$ gates, the unitary weight matrix that it computes ensures significant cross-talk between $2^n$ features.</span></span>

![Snabbt Entangling Quantum-krets på 5 qubits (med två cykliska lager).](~/media/5-qubit-qccc.png)

<span data-ttu-id="8124f-134">Kretsen i exemplet ovan består av 6 engångs-qubit-grindar $ (G_1, \ldots, G_5; G_ {16} ) $ och 10 2-qubits Gates $ (G_6, \ldots, G_ {15} ) $.</span><span class="sxs-lookup"><span data-stu-id="8124f-134">The circuit in the above example consists of 6 single-qubit gates $(G_1,\ldots,G_5; G_{16})$ and 10 two-qubits gates $(G_6,\ldots,G_{15})$.</span></span> <span data-ttu-id="8124f-135">Förutsatt att varje port har definierats med en enkel parameter kan vi använda 16 parametrar, medan dimensionen för 5-qubit Hilbert-utrymmet är 32.</span><span class="sxs-lookup"><span data-stu-id="8124f-135">Assuming that each of the gates is defined with one learnable parameter we have 16 learnable parameters, while the dimension of the 5-qubit Hilbert space is 32.</span></span> <span data-ttu-id="8124f-136">Sådan krets geometri kan enkelt generaliseras till alla $n $-qubit-register, när $n $ är udda, vilket ger kretsar med $3 n + 1 $ parametrar för $2 ^ n $-dimensionellt funktions utrymme.</span><span class="sxs-lookup"><span data-stu-id="8124f-136">Such circuit geometry can be easily generalized to any $n$-qubit register, when $n$ is odd, yielding circuits with $3 n+1$ parameters for $2^n$-dimensional feature space.</span></span>

## <a name="classifier-training-as-a-supervised-learning-task"></a><span data-ttu-id="8124f-137">Klassificerings utbildning som en övervakad utbildnings uppgift</span><span class="sxs-lookup"><span data-stu-id="8124f-137">Classifier training as a supervised learning task</span></span>

<span data-ttu-id="8124f-138">Utbildning av en klassificerings modell omfattar att hitta optimala värden för dess operativa parametrar, så att de maximerar det genomsnittliga sannolikheten för att kunna härleda rätt utbildnings etiketter i övnings exemplen.</span><span class="sxs-lookup"><span data-stu-id="8124f-138">Training of a classifier model involves finding optimal values of its operational parameters, such that they maximize the average likelihood of inferring the correct training labels across the training samples.</span></span>
<span data-ttu-id="8124f-139">Här gäller endast skapar med två nivå klassificering, d.v.s. $d = $2 och bara två klasser med etiketterna $y _1, y_2 $.</span><span class="sxs-lookup"><span data-stu-id="8124f-139">Here, we concern ourselves with two level classification only, i.e. the case of $d=2$ and only two classes with the labels $y_1,y_2$.</span></span>

> [!NOTE]
> <span data-ttu-id="8124f-140">Ett allmänt sätt att generalisera våra metoder för att använda ett godtyckligt antal klasser är att ersätta qubits med qudits, d.v.s. Quantum units med $d $ bas tillstånd och det tvåvägs måttet med $d $-Way-mätning.</span><span class="sxs-lookup"><span data-stu-id="8124f-140">A principled way of generalizing our methods to arbitrary number of classes is to replace qubits with qudits, i.e. quantum units with $d$ basis states, and the two-way measurement with $d$-way measurement.</span></span>

### <a name="likelihood-as-the-training-goal"></a><span data-ttu-id="8124f-141">Sannolikhet för utbildnings målet</span><span class="sxs-lookup"><span data-stu-id="8124f-141">Likelihood as the training goal</span></span>

<span data-ttu-id="8124f-142">Med en inlärtd Quantum-krets $U (\theta) $, där $ \theta $ är en Vector med parametrar och som anger den slutliga mätningen med $M $, är det genomsnittliga sannolikheten att den korrekta etikettens härledning är $ $ \begin{align} \mathcal{L} (\theta) = \frac {1} {| \mathcal{D} |} \left (\ sum_ {(x, y_1) \In\mathcal{D}} P (M = y_1 | U (\theta) x) + \ sum_ {(x, y_2) \in\mathcal{D}} P (M = y_2 | U (\theta) x) \right) \end{align} $ $ där $P (M = y | z) $ är sannolikheten att mäta $y $ i Quantum State $z $.</span><span class="sxs-lookup"><span data-stu-id="8124f-142">Given a learnable quantum circuit $U(\theta)$, where $\theta$ is a vector of parameters, and denoting the final measurement by $M$, the average likelihood of the correct label inference is $$ \begin{align} \mathcal{L}(\theta)=\frac{1}{|\mathcal{D}|} \left( \sum_{(x,y_1)\in\mathcal{D}} P(M=y_1|U(\theta) x) + \sum_{(x,y_2)\in\mathcal{D}} P(M=y_2|U(\theta) x)\right) \end{align} $$ where $P(M=y|z)$ is the probability of measuring $y$ in quantum state $z$.</span></span>
<span data-ttu-id="8124f-143">Här är det tillräckligt för att förstå att sannolikhets funktionen $ \mathcal{L} (\theta) $ är smidig i $ \theta $ och dess derivat i $ \ theta_j $ kan beräknas av i stort sett samma Quantum-protokoll som används för att beräkna sannolikhets funktionen.</span><span class="sxs-lookup"><span data-stu-id="8124f-143">Here, it suffices to understand that the likelihood function $\mathcal{L}(\theta)$ is smooth in $\theta$ and its derivative in any $\theta_j$ can be computed by essentially the same quantum protocol as used for computing the likelihood function itself.</span></span> <span data-ttu-id="8124f-144">Detta möjliggör optimering av $ \mathcal{L} (\theta) $ med tonings brantaste.</span><span class="sxs-lookup"><span data-stu-id="8124f-144">This allows for optimizing the $\mathcal{L}(\theta)$ by gradient descent.</span></span>

### <a name="classifier-bias-and-training-score"></a><span data-ttu-id="8124f-145">Klassificerings resultat för klassificering och utbildning</span><span class="sxs-lookup"><span data-stu-id="8124f-145">Classifier bias and training score</span></span>

<span data-ttu-id="8124f-146">Med några av de mellanliggande (eller slutliga) värdena för parametrarna i $ \theta $, måste vi identifiera ett enkelt reellt värde $b $ known som *klassificering av klassificerare* .</span><span class="sxs-lookup"><span data-stu-id="8124f-146">Given some intermediate (or final) values of the parameters in $\theta$, we need to identify a single real value $b$ know as *classifier bias* to do the inference.</span></span> <span data-ttu-id="8124f-147">Etikettens härlednings regel fungerar på följande sätt:</span><span class="sxs-lookup"><span data-stu-id="8124f-147">The label inference rule works as follows:</span></span> 
- <span data-ttu-id="8124f-148">Ett exempel på $x $ tilldelas etikett $y _2 $ om och endast om $P (M = y_2 | U (\theta) x) + b > $0,5 (regel 1) (annars tilldelas etiketten $y _1 $)</span><span class="sxs-lookup"><span data-stu-id="8124f-148">A sample $x$ is assigned label $y_2$ if and only if $P(M=y_2|U(\theta) x) + b > 0.5$  (RULE1) (otherwise it is assigned label $y_1$)</span></span>

<span data-ttu-id="8124f-149">Tydligt $b $ måste vara i intervallet $ (-0,5, + 0,5) $ för att vara meningsfull.</span><span class="sxs-lookup"><span data-stu-id="8124f-149">Clearly $b$ must be in the interval $(-0.5,+0.5)$ to be meaningful.</span></span>

<span data-ttu-id="8124f-150">En utbildnings väska $ (x, y) \in \mathcal{D} $ betraktas som en *felklassificering* med förskjutningen $b $ om etiketten som härleds till $x $ per regel 1 faktiskt skiljer sig från $y $.</span><span class="sxs-lookup"><span data-stu-id="8124f-150">A training case $(x,y) \in \mathcal{D}$ is considered a *misclassification* given the bias $b$ if the label inferred for $x$ as per RULE1 is actually different from $y$.</span></span> <span data-ttu-id="8124f-151">Det totala antalet felklassificeringar är *inlärnings poängen* för klassificeraren med hänsyn till bias $b $.</span><span class="sxs-lookup"><span data-stu-id="8124f-151">The overall number of misclassifications is the *training score* of the classifier given the bias $b$.</span></span> <span data-ttu-id="8124f-152">Den *optimala* klassificerings bias $b $ minimerar utbildnings poängen.</span><span class="sxs-lookup"><span data-stu-id="8124f-152">The *optimal* classifier bias $b$ minimizes the training score.</span></span> <span data-ttu-id="8124f-153">Det är enkelt att se att den beräknade sannolikheten $ \{ P (M = y_2 | U (\theta) x) | (x, \*) \in\mathcal{D} \} $, den optimala klassificeraren kan hittas av binär sökning i intervall $ (-0,5, + 0,5) $ genom att göra högst $ \ log_2 (| \mathcal{D} |) $ steg.</span><span class="sxs-lookup"><span data-stu-id="8124f-153">It is easy to see that, given the precomputed probability estimates $\{ P(M=y_2|U(\theta) x) | (x,\*)\in\mathcal{D} \}$, the optimal classifier bias can be found by binary search in interval $(-0.5,+0.5)$ by making at most $\log_2(|\mathcal{D}|)$ steps.</span></span>

### <a name="reference"></a><span data-ttu-id="8124f-154">Referens</span><span class="sxs-lookup"><span data-stu-id="8124f-154">Reference</span></span>

<span data-ttu-id="8124f-155">Den här informationen bör vara tillräckligt stor för att börja spela med koden.</span><span class="sxs-lookup"><span data-stu-id="8124f-155">This information should be enough to start playing with the code.</span></span> <span data-ttu-id="8124f-156">Men om du vill lära dig mer om den här modellen läser du det ursprungliga förslaget: [ *"kretsbaserade Quantum-klassificerare", Maria Schuld, Alex Bocharov, krysta Svore och Nathan Wiebe*](https://arxiv.org/abs/1804.00633)</span><span class="sxs-lookup"><span data-stu-id="8124f-156">However, if you want to learn more about this model, please read the original proposal: [*'Circuit-centric quantum classifiers', Maria Schuld, Alex Bocharov, Krysta Svore and Nathan Wiebe*](https://arxiv.org/abs/1804.00633)</span></span>

<span data-ttu-id="8124f-157">Förutom kod exemplet som visas i nästa steg kan du också börja utforska Quantum-klassificeringen i [den här självstudien](https://github.com/microsoft/QuantumKatas/tree/main/tutorials/QuantumClassification)</span><span class="sxs-lookup"><span data-stu-id="8124f-157">In addition to the code sample you will see in the next steps, you can also start exploring quantum classification in [this tutorial](https://github.com/microsoft/QuantumKatas/tree/main/tutorials/QuantumClassification)</span></span> 
